{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-708d46d3f9180abe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Multimodal Data Fusion - Project Work: Multi-Modal Physical Exercise Classification\n",
    "\n",
    "\n",
    "In this project, real multi-modal data is studied by utilizing different techniques presented during the course. In addition, there is an optional task to try some different approaches to identify persons from the same dataset. Open MEx dataset from UCI machine learning repository is used. Idea is to apply different techniques to recognize physical exercises from wearable sensors and depth camera, user-independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "Add your information here\n",
    "\n",
    "Name:\n",
    "\n",
    "Student number:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32738734cf6f1a4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Description \n",
    "\n",
    "The goal of this project is to develop user-independent pre-processing and classification models to recognize 7 different physical exercises measured by accelerometer (attached to subject's thigh) and depth camera (above the subject facing downwards recording an aerial view). All the exercises were performed subject lying down on the mat. Original dataset have also another acceleration sensor and pressure-sensitive mat, but those two modalities are ommited in this project. There are totally 30 subjects in the original dataset, and in this work subset of 10 person is utilized. Detailed description of the dataset and original data can be access in [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#). We are providing the subset of dataset in Moodle.\n",
    "\n",
    "The project work is divided on following phases:\n",
    "\n",
    "1. Data preparation, exploration, and visualization\n",
    "2. Feature extraction and unimodal fusion for classification\n",
    "3. Feature extraction and feature-level fusion for multimodal classification\n",
    "4. Decision-level fusion for multimodal classification\n",
    "5. Bonus task: Multimodal biometric identification of persons\n",
    "\n",
    "where 1-4 are compulsory (max. 10 points each), and 5 is optional to get bonus points (max. 5+5 points). In each phase, you should visualize and analyse the results and document the work and findings properly by text blocks and figures between the code. <b> Nice looking </b> and <b> informative </b> notebook representing your results and analysis will be part of the grading in addition to actual implementation.\n",
    "\n",
    "The results are validated using confusion matrices and F1 scores. F1 macro score is given as \n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{equation}\n",
    "F1_{macro} = \\frac{1}{N} \\sum_i^N F1_i,\n",
    "\\end{equation}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "where $F1_i = 2  \\frac{precision_i * recall_i}{precision_i + recall_i}$, and $N$ is the number of classes.\n",
    "<br>\n",
    "\n",
    "## Learning goals \n",
    "\n",
    "After the project work, you should  \n",
    "\n",
    "- be able to study real world multi-modal data\n",
    "- be able to apply different data fusion techniques to real-world problem\n",
    "- be able to evaluate the results\n",
    "- be able to analyse the outcome\n",
    "- be able to document your work properly\n",
    "\n",
    "## Relevant lectures\n",
    "\n",
    "Lectures 1-8\n",
    "\n",
    "## Relevant exercises\n",
    "\n",
    "Exercises 0-6\n",
    "\n",
    "## Relevant chapters in course book\n",
    "\n",
    "Chapter 1-14\n",
    "\n",
    "## Additional Material \n",
    "\n",
    "* Original dataset [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#)\n",
    "* Related scientific article [MEx: Multi-modal Exercises Dataset for Human Activity Recognition](https://arxiv.org/pdf/1908.08992.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e096db2d9e8c24e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 1. Data preparation, exploration, and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 1.</b>\n",
    "\n",
    "Download data from the Moodle's Project section. Get yourself familiar with the folder structure and data. You can read the data files using the function given below. Each file consists one exercise type performed by single user. Data are divided on multiple folders. Note that, in each folder there is one long sequence of single exercise, except exercise 4 which is performed two times in different ways. Those two sequences belongs to same class. Do the following subtasks to pre-analyse data examples and to prepare the training and testing data for next tasks:\n",
    "<br>\n",
    "<br> \n",
    "<p> <b>1.1</b> Read raw data from the files. Prepare and divide each data file to shorter sequences using windowing method. Similar to related article \"MEx: Multi-modal Exercises Dataset for Human Activity Recognition\", use 5 second window and 3 second overlapping between windows, producing several example sequences from one exercise file for classification purposes. Windowing is working so that starting from the beginning of each long exercise sequence, take 5 seconds of data points (from synchronized acceleration data and depth images) based on the time stamps. Next, move the window 2 seconds forward and take another 5 seconds of data. Then continue this until your are at the end of sequence. Each window will consists 500x3 matrix of acceleration data and 5x192 matrix of depth image data.</p>\n",
    "<br>  \n",
    "<p> <b>1.2</b> Plot few examples of prepared data for each modalities (accelometer and depth camera). Plot acceleration sensor as multi-dimensional time-series and depth camera data as 2D image. Plot 5 second acceleration sensor and depth image sequences of person 1 and 5 performing exercises 2, 5, and 6. Take the first windowed example from the long exercise sequence. </p>\n",
    "<br>\n",
    "<p> <b>1.3</b> Split the prepared dataset to training and testing datasets so that data of persons 1-7 are used for training and data of persons 8-10 are used for testing. In next tasks, training dataset could be further divided on (multiple) validation data folds to tune the models parameters, when needed.<br>\n",
    "<br> \n",
    "Document your work, calculate the indicator statistics of training and testing datasets (number of examples, dimensions of each example) and visualize prepared examples.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir,getcwd\n",
    "    \n",
    "#Reads data from the folders\n",
    "\n",
    "def read_data_from_folders(folder_name, ID, outputform):\n",
    "    output = np.empty(outputform, dtype=object)\n",
    "    \n",
    "    for n in range(len(ID)):\n",
    "        #check files in folder.\n",
    "        currentDir = getcwd()\n",
    "        filepath = \"%s/%s/%02d\" %(currentDir, \"MEx/\" + folder_name, ID[n])\n",
    "        files = sorted(listdir(filepath))\n",
    "        count = 0\n",
    "        \n",
    "        for f in files:\n",
    "            file = filepath + \"/\" + f\n",
    "            data = pd.read_csv(file, delimiter=',', header=None)\n",
    "            output[n,count] = data \n",
    "            count += 1\n",
    "            \n",
    "    return output\n",
    "\n",
    "folders = ['act', 'dc_0.05_0.05']\n",
    "\n",
    "allData = np.empty((2,10,8),dtype=object)\n",
    "num = 0\n",
    "ids = np.arange(1,11)\n",
    "\n",
    "for idx,fol in enumerate(folders):\n",
    "    allData[idx,:,:]=read_data_from_folders(fol, ids, (10,8))\n",
    "print(allData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "(30, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "def window_split_file(data,miliseconds=5000):\n",
    "    step_size=int(data.iloc[1,0]-data.iloc[0,0])\n",
    "#     print(\"The last timestamp\",data.iloc[-1,0])\n",
    "#     print(\"Step size\",step_size)\n",
    "    nr_points=int(miliseconds/step_size)\n",
    "    new_data=[]\n",
    "    \n",
    "    stop_time=int(data.iloc[-1,0]-miliseconds)\n",
    "#     print(\"Stop time\",stop_time/2000)\n",
    "    out=np.empty((int(stop_time/2000)+1,int(miliseconds/step_size),data.shape[1]-1))\n",
    "#     print(\"Out\",out.shape)\n",
    "    count=0\n",
    "    for start_time in range(0,stop_time,2000):\n",
    "        start_index=int(start_time/step_size)\n",
    "        stop_index=start_index+nr_points\n",
    "#         print(\"Start\",start_index)\n",
    "#         print(\"End point\",stop_index)\n",
    "        out[count]=data.iloc[start_index:stop_index,1:]\n",
    "        count+=1\n",
    "    return out\n",
    "def apply_windowing(data,outputform):\n",
    "    outData=np.empty((2,10,8),dtype=object)\n",
    "    count=0\n",
    "    for sensor in data:\n",
    "        print(\"Done\")\n",
    "        output = np.empty(outputform, dtype=object)\n",
    "      \n",
    "        for p_count,person in enumerate(sensor):\n",
    "            \n",
    "            for f_count,file in enumerate(person):\n",
    "                \n",
    "                output[p_count,f_count]=window_split_file(sensor[p_count,f_count])\n",
    "        outData[count]=output\n",
    "        count+=1\n",
    "    return outData\n",
    "windowed_data=apply_windowing(allData,(10,8))\n",
    "\n",
    "print(windowed_data[0,0,0].shape)\n",
    "# print(r[1][0,0].shape)\n",
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Plot few examples of prepared data for each modalities (accelometer and depth camera). Plot acceleration sensor as multi-dimensional time-series and depth camera data as 2D image. Plot 5 second acceleration sensor and depth image sequences of person 1 and 5 performing exercises 2, 5, and 6. Take the first windowed example from the long exercise sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 5, 192)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd78c7c9490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############    windowed_data[0]                              [0,1]                       [0]   \n",
    "##############        0 for acc,1 for camera        person 1, exercise 2                  first frame of the exercise\n",
    "import matplotlib.pyplot as plt\n",
    "t=range(0,500)\n",
    "f = plt.figure(figsize=(30,10))\n",
    "f.add_subplot(3, 3, 1)\n",
    "plt.plot(t,windowed_data[0][0,1][0], 'g')\n",
    "plt.plot(t,windowed_data[0][4,1][0], 'r')\n",
    "\n",
    "f.add_subplot(3, 3, 2)\n",
    "plt.plot(t,windowed_data[0][0,4][0], 'g')\n",
    "plt.plot(t,windowed_data[0][4,4][0], 'r')\n",
    "\n",
    "f.add_subplot(3, 3, 3)\n",
    "plt.plot(t,windowed_data[0][0,5][0], 'g')\n",
    "plt.plot(t,windowed_data[0][4,5][0], 'r')\n",
    "\n",
    "f.add_subplot(3, 3, 4)\n",
    "# print(windowed_data[1][4,5][0,0].shape)\n",
    "image=windowed_data[1][0,1][0,0].reshape((12,16))\n",
    "plt.imshow(image)\n",
    "f.add_subplot(3, 3, 7)\n",
    "image=windowed_data[1][4,1][0,0].reshape((12,16))\n",
    "plt.imshow(image)\n",
    "\n",
    "f.add_subplot(3, 3, 5)\n",
    "# print(windowed_data[1][4,5][0,0].shape)\n",
    "image=windowed_data[1][0,4][0,0].reshape((12,16))\n",
    "plt.imshow(image)\n",
    "f.add_subplot(3, 3, 8)\n",
    "image=windowed_data[1][4,4][0,0].reshape((12,16))\n",
    "plt.imshow(image)\n",
    "\n",
    "f.add_subplot(3, 3, 6)\n",
    "print(windowed_data[1][0,5].shape)\n",
    "image=windowed_data[1][0,5][0,0].reshape((12,16))\n",
    "plt.imshow(image)\n",
    "f.add_subplot(3, 3, 9)\n",
    "image=windowed_data[1][4,5][0,0].reshape((12,16))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Split the prepared dataset to training and testing datasets so that data of persons 1-7 are used for training and data of persons 8-10 are used for testing. In next tasks, training dataset could be further divided on (multiple) validation data folds to tune the models parameters, when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out shape: (1487, 500, 3)  Label shape: (1487,)\n",
      "Out shape: (598, 500, 3)  Label shape: (598,)\n",
      "Out shape: (1486, 5, 192)  Label shape: (1486,)\n",
      "Out shape: (598, 5, 192)  Label shape: (598,)\n"
     ]
    }
   ],
   "source": [
    "def create_data_set(data):\n",
    "    out_data=np.empty((0,data[0,0].shape[1],data[0,0].shape[2]))\n",
    "    out_labels=np.empty(0)\n",
    "    for idx,person in enumerate(data):\n",
    "        for idy,exercise in enumerate(person):\n",
    "#             print(exercise.shape)\n",
    "            out_data=np.concatenate((out_data,exercise))\n",
    "            labels=np.full(exercise.shape[0],idy+1)### create labels array with the id of the exercise\n",
    "            out_labels=np.concatenate((out_labels,labels))\n",
    "    print(\"Out shape:\",out_data.shape,\" Label shape:\",out_labels.shape)\n",
    "    return out_data, out_labels\n",
    "training_data_slice=windowed_data[:,:7,:]\n",
    "testing_data_slice=windowed_data[:,7:,:]\n",
    "\n",
    "training_data_acc=training_data_slice[0,:,:]\n",
    "\n",
    "x_train_acc,y_train_acc=create_data_set(training_data_slice[0])\n",
    "x_test_acc,y_test_acc=create_data_set(testing_data_slice[0])\n",
    "x_train_cam,y_train_cam=create_data_set(training_data_slice[1])\n",
    "x_test_cam,y_test_cam=create_data_set(testing_data_slice[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature extraction and fusion for unimodal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 2.</b>\n",
    "\n",
    "Use the training dataset prepared in task 1. to build models based on the combination of principal component analysis (PCA), linear discriminant analysis (LDA), and nearest neighbour (NN) classifier for each modality separately and evaluate the model on test dataset. Do the subtasks given as\n",
    "<br>\n",
    "<br>\n",
    "<p> <b>2.1</b> Calculate PCA and LDA transformations to reduce the dimensionality of accelerometer data (e.g., using scikit-learn implementations). Before transformations downsample data from 100 Hz to 25 Hz (using scipy.signal.resample) to get 125x3 matrix of data for each 5 sec window. You should also standardize the values to zero mean and unit variance before the transformations. Using training dataset, fit PCA with 5-dimensional subspace (i.e., choosing the 5 largest principal components) and fit LDA with 5-dimensional subspace. Transform both train and test examples to this low-dimensional feature representation. Concatenate each sequence to single vector size of 3x(5+5). Perform the fusion of PCA and LDA similar manner as presented in Lecture 3 (pages-19-20) using NN method. Evaluate the performance on testset. Show confusion matrix and F1 scores of the results. </p>\n",
    "<br>\n",
    "<p> <b>2.2</b> Use PCA and LDA transformations to reduce the dimensionality of depth images. You should also standardize the values to zero mean and unit variance before the transformations. Fit PCA and LDA for all training images (12x16, 192-dimensional in vectorized form) by choosing 5-dimensional subspace for both PCA and LDA. Transform both train and test examples to this low-dimensional feature representation. Concatenate each sequence to single vector size of 5x1x(5+5). Similar to task 2.1, do the PCA and LDA fusion using NN and evaluate the performance on testset. Show confusion matrix and F1 scores of the results. </p>\n",
    "<br> \n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 2.1-2.2.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 3)\n",
      "(125, 3)\n",
      "(1487, 125, 3)\n",
      "[-0.75820461 -1.51519852  0.18090713]\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "def downsample(data):\n",
    "    m=signal.resample(data,125)\n",
    "    return m\n",
    "def standardise(data):\n",
    "    mean=np.mean(data,axis=0)\n",
    "    std=np.std(data,axis=0)\n",
    "    print(mean.shape)\n",
    "    print(std.shape)\n",
    "    return (data-mean)/std\n",
    "def preprocess(data):\n",
    "    aux=np.apply_along_axis(downsample, 1, x_train_acc)\n",
    "    return standardise(aux)\n",
    "x_train_acc=preprocess (x_train_acc)\n",
    "\n",
    "print(x_train_acc.shape)\n",
    "# print(x_train_acc[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature extraction and feature-level fusion for multimodal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 3.</b>\n",
    "\n",
    "Prepare new feature sets for each modality and combine them to single feature representation. Compare two classifiers from scikit-learn. Train classifiers using joint feature presentation. Evaluate and compare the result using testing dataset. Do the subtasks given as\n",
    "<br>   \n",
    "<br> \n",
    "<p> <b>3.1</b> Similar to task 2.1, calculate PCA for accelerometer, but choose now the 10 largest principal components as 10-dim feature vector for each window. In addition, for each window calculate mean and standard deviation of each three acc channels as statistical features, resulting 6-dimensional vector. Combine these to 36-dimensional final feature vector.</p>\n",
    "<br>  \n",
    "<p> <b>3.2</b> Similar to task 2.2, calculate the PCA for depth images using same setup, but now choose the 10 largest principal components as feature vector. Concatenate the image sequence forming 50-dimensional feature vector from each windowed example.</p>\n",
    "<br> \n",
    "<p> <b>3.3</b> Form a joint feature presentation of features extracted in 3.1 and 3.2, resulting 86-dimensional feature vector for each example. Normalize data between 0-1 using the training dataset. Use support vector machine (SVM) with RBF-kernel and Gaussian naiveBayes classifier (use default parameter values for both classifiers). Train the classifiers and evaluate and compare classifiers on testset using confusion matrices and F1 scores.</p>\n",
    "<br> \n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 3.1-3.3.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decision-level fusion for multimodal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 4.</b>\n",
    "\n",
    "Use features calculated for each modality in task 3. Choose base classifier for each modality from scikit-learn. Train classifiers for each modality feature presentations separately and combine the outputs in decision level. Evaluate and compare the result on testing dataset. Do the subtasks given as\n",
    "<br>\n",
    "<br> \n",
    "<p> <b>4.1</b> Use base classifiers of support vector machine (SVM) with RBF-kernel and AdaBoost classifier (with random_state=0). \n",
    "Normalize data between 0-1 using the training dataset. Train the base classifiers by tuning the model parameters (<i>C</i> parameter and RBF-kernel <i>gamma</i> in SVM as well as <i>n_estimators</i> and <i>learning_rate</i> in Adaboost) using 10-fold cross-validation on training dataset to find optimal set of parameters (hint: use GridSearchCV from scikit-learn). For grid search use the following values $C = [0.1, 1.0, 10.0, 100.0]$, $gamma=[0.1, 0.25, 0.5, 0.75, 1.0, 2.0]$, $n\\_estimators = [50, 100, 500, 1000]$, and $learning\\_rate = [0.1, 0.25, 0.5, 0.75,1.0]$. Choose the best parameters and train the classifiers for each modality on whole training dataset. Is there a possibility that classifiers will overfit to training data using this parameter selection strategy? If so, why? </p>\n",
    "<br>\n",
    "<p> <b>4.2</b> Predict probabilistic outputs of each trained classifier for both modalities using the test set. </p>\n",
    "<br>\n",
    "<p> <b>4.3</b> Combine the probabilistic outputs of different modalities by fixed classification rules: max, min, prod, and sum. Evaluate, compare, and analyse the final combined results using confusion matrices and F1 scores. Show results for each base classifier combinations (i.e., $SVM_{acc}+SVM_{depth}$, $AdaBoost_{acc}+AdaBoost_{depth}$, $SVM_{acc}+AdaBoost_{depth}$, $AdaBoost_{acc}+SVM_{depth}$)</p>\n",
    "<br>\n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 4.1-4.3.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bonus task: Multimodal biometric identification of persons (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 5.</b>\n",
    "\n",
    "Can you build a classifier that recognizes the person who is performing the exercise? Use same 10 person dataset and split it so that first 25% of each long exercise sequence is used for training and rest 75% of each sequence is used for testing the classifier. Use same 5 second windowing with 3 seconds overlap to prepare the examples. Note that, now the person identity is the class label instead of exercise type. Max. 10 points are given but you can earn points from partial solution, as well.\n",
    "<br> \n",
    "<br> \n",
    "<p> <b>5.1</b> Build a classifier to identify persons based on the features and one of the models given in task 4 (max. 5 points).</p>\n",
    "<br> \n",
    "<p> <b>5.2</b> Can you build your own solution (using new features, new classification model or different fusion approaches) to beat the approach in Task 5.1 ? (max. 5 points) </p>\n",
    "<br>  \n",
    "Document your work. Evaluate and compare the results using confusion matrix and F1 score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
